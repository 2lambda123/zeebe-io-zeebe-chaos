"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[216],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return h}});var n=a(7294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=c(a),h=l,m=u["".concat(s,".").concat(h)]||u[h]||d[h]||r;return a?n.createElement(m,o(o({ref:t},p),{},{components:a})):n.createElement(m,o({ref:t},p))}));function h(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=a.length,o=new Array(r);o[0]=u;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:l,o[1]=i;for(var c=2;c<r;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},9878:function(e,t,a){a.r(t),a.d(t,{assets:function(){return p},contentTitle:function(){return s},default:function(){return h},frontMatter:function(){return i},metadata:function(){return c},toc:function(){return d}});var n=a(7462),l=a(3366),r=(a(7294),a(3905)),o=["components"],i={layout:"posts",title:"Handling of Big Variables",date:new Date("2022-01-19T00:00:00.000Z"),categories:["chaos_experiment","bpmn","variables"],tags:["availability"],authors:"zell"},s="Chaos Day Summary",c={permalink:"/zeebe-chaos/2022/01/19/big-variables",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2022-01-19-big-variables/index.md",source:"@site/blog/2022-01-19-big-variables/index.md",title:"Handling of Big Variables",description:"New Year;New Chaos",date:"2022-01-19T00:00:00.000Z",formattedDate:"January 19, 2022",tags:[{label:"availability",permalink:"/zeebe-chaos/tags/availability"}],readingTime:5.29,truncated:!0,authors:[{name:"Christopher Zell",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],prevItem:{title:"High Snapshot Frequency",permalink:"/zeebe-chaos/2022/02/01/High-Snapshot-Frequency"},nextItem:{title:"Worker count should not impact performance",permalink:"/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance"}},p={authorsImageUrls:[void 0]},d=[{value:"Chaos Experiment",id:"chaos-experiment",children:[{value:"Expected",id:"expected",children:[]},{value:"Actual",id:"actual",children:[]},{value:"Camunda Cloud",id:"camunda-cloud",children:[]},{value:"Result",id:"result",children:[]},{value:"Found Bugs",id:"found-bugs",children:[]}]},{value:"Message pack is not valid",id:"message-pack-is-not-valid",children:[]},{value:"Configure the Starter payload",id:"configure-the-starter-payload",children:[]}],u={toc:d};function h(e){var t=e.components,i=(0,l.Z)(e,o);return(0,r.kt)("wrapper",(0,n.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"New Year;\ud83c\udf89New Chaos\ud83d\udc12"),(0,r.kt)("p",null,'This time I wanted to experiment with "big" variables. Zeebe supports a ',(0,r.kt)("inlineCode",{parentName:"p"},"maxMessageSize")," of 4 MB, which is quite big. In general, it should be clear that using big variables will cause performance issues, but today I also want to find out whether the system can handle big variables (~1 MB) at all. "),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"TL;DR;")," Our Chaos experiment failed! Zeebe and Camunda Cloud is not able to handle (per default) big variables (~1 MB) without issues."),(0,r.kt)("h2",{id:"chaos-experiment"},"Chaos Experiment"),(0,r.kt)("p",null,"Normally we run our benchmarks with ~32 KB payload size. This time we want to try out a payload size of ~1 MB and verify whether the system can handle such payload sizes. The payload we use can be found ",(0,r.kt)("a",{parentName:"p",href:"pathname://big_payload.json"},"here"),". "),(0,r.kt)("p",null,"The benchmark setup, is similar to default Zeebe benchmarks you can find ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks/setup/default"},"here"),". To make it work and fair we updated the starter and worker resources for both, base and the chaos cluster."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-diff"},'diff --git a/benchmarks/setup/default/starter.yaml b/benchmarks/setup/default/starter.yaml\nindex 78c6e81dbb..d0404d4d3e 100644\n--- a/benchmarks/setup/default/starter.yaml\n+++ b/benchmarks/setup/default/starter.yaml\n@@ -30,11 +30,11 @@ spec:\n             value: "warn"\n         resources:\n           limits:\n-            cpu: 250m\n-            memory: 256Mi\n+            cpu: 1G\n+            memory: 2Gi\n           requests:\n-            cpu: 250m\n-            memory: 256Mi\n+            cpu: 1G\n+            memory: 2Gi\n ---\n apiVersion: v1\n kind: Service\ndiff --git a/benchmarks/setup/default/worker.yaml b/benchmarks/setup/default/worker.yaml\nindex cd6f5ffeb6..05b195291f 100644\n--- a/benchmarks/setup/default/worker.yaml\n+++ b/benchmarks/setup/default/worker.yaml\n@@ -31,11 +31,11 @@ spec:\n             value: "warn"\n         resources:\n           limits:\n-            cpu: 500m\n-            memory: 256Mi\n+            cpu: 1G\n+            memory: 1Gi\n           requests:\n-            cpu: 500m\n-            memory: 256Mi\n+            cpu: 1G\n+            memory: 1Gi\n')),(0,r.kt)("h3",{id:"expected"},"Expected"),(0,r.kt)("p",null,"It is expected that the performance will drop, we formulate the following hypothesis."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Hypothesis: With a bigger payload size of e.g. 1 MB, Zeebe should be still able to handle process instances, maybe under a degraded performance, but in general the availability must not suffer from such a payload size.")),(0,r.kt)("h3",{id:"actual"},"Actual"),(0,r.kt)("h4",{id:"base"},"Base"),(0,r.kt)("p",null,"We started a base benchmark with ~32 KB to verify how it looks like normally."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"base",src:a(4607).Z})),(0,r.kt)("h4",{id:"small-payload"},"Small Payload"),(0,r.kt)("p",null,"In order to verify how Zeebe handles different payload, we first started with a small payload ~130 bytes, which is part of the Starter application (called ",(0,r.kt)("inlineCode",{parentName:"p"},"small_payload.json"),"). "),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"small-payload",src:a(5082).Z})),(0,r.kt)("p",null,"We can see that the system handles such payload without any issues, and we can reach ~190 process instances per second (PI/s)."),(0,r.kt)("h4",{id:"big-payload"},"Big Payload"),(0,r.kt)("p",null,"After running with a small payload, we changed the payload to a size of ~1 MB. This immediately broke the standalone gateways."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"big-payload",src:a(3506).Z})),(0,r.kt)("p",null,"The gateways went out of memory (OOM) in a loop. No processing was made in this time."),(0,r.kt)("h5",{id:"increasing-resources"},"Increasing Resources"),(0,r.kt)("p",null,"In order to continue the experiment and to verify how Zeebe itself can handle it, we increased the gateway resources."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-diff"},"diff --git a/benchmarks/setup/default/zeebe-values.yaml b/benchmarks/setup/default/zeebe-values.yaml\nindex 371ba538dc..7a11c10366 100644\n--- a/benchmarks/setup/default/zeebe-values.yaml\n+++ b/benchmarks/setup/default/zeebe-values.yaml\n@@ -38,10 +38,10 @@ gateway:\n   resources:\n     limits:\n       cpu: 1\n-      memory: 512Mi\n+      memory: 4Gi\n     requests:\n       cpu: 1\n-      memory: 512Mi\n+      memory: 4Gi\n")),(0,r.kt)("p",null,"But this doesn't help. The gateway went no longer OOM, but it was still not able to handle the payload."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"increase-res",src:a(3181).Z})),(0,r.kt)("p",null,'We can see that in a short period of time some events have been processed (small spike in the "Current Events" panel), but this stopped quite fast again. In the gateway logs there are endless warnings:'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'Warning 2022-01-20 10:09:32.644 CET zeebe-cluster-helm "Stream Error"\nWarning 2022-01-20 10:09:56.847 CET zeebe-cluster-helm "Stream Error"\n')),(0,r.kt)("p",null,"With an underlying exception: ",(0,r.kt)("inlineCode",{parentName:"p"},"io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place")," "),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Stacktrace"),"``` io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:172) ~[netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:481) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:105) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:357) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:1007) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:963) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:515) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:521) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.Http2ConnectionHandler.closeStream(Http2ConnectionHandler.java:613) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onRstStreamRead(DefaultHttp2ConnectionDecoder.java:444) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onRstStreamRead(Http2InboundFrameLogger.java:80) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readRstStreamFrame(DefaultHttp2FrameReader.java:509) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:259) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:159) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:173) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:378) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:438) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:510) [netty-codec-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:449) [netty-codec-4.1.73.Final.jar:4.1.73.Final] at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:279) [netty-codec-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final] at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.73.Final.jar:4.1.73.Final] at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.73.Final.jar:4.1.73.Final] at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.73.Final.jar:4.1.73.Final] at java.lang.Thread.run(Unknown Source) ```"),(0,r.kt)("p",null,"On the client side we can see that the Zeebe cluster seems to be unavailable."),(0,r.kt)("h3",{id:"camunda-cloud"},"Camunda Cloud"),(0,r.kt)("p",null,"We wanted to verify how Camunda Cloud and our standard Cluster plan (GA Hardware Plan) handles such a payload. But the result was the same."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"cc-general",src:a(1112).Z})),(0,r.kt)("p",null,"The processing stopped quite fast due to OOM of the gateway. We can see that operate is also not able to handle such load."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"failed-op",src:a(1924).Z})),(0,r.kt)("p",null,"In our console overview we see that all services (exception Zeebe) went unhealthy"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"console-healthy",src:a(83).Z})),(0,r.kt)("h3",{id:"result"},"Result"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("em",{parentName:"p"},"Hypothesis: With a bigger payload size of e.g. 1 MB Zeebe, should be still able to handle process instances, maybe under a degraded performance but in general the availability must not suffer from such a payload size."))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"We were not able to validate our hypothesis, which means our chaos experiment failed!")," \ud83d\udca5"),(0,r.kt)("h3",{id:"found-bugs"},"Found Bugs"),(0,r.kt)("p",null,"We opened the following bug issues:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Gateway can't handle bigger payload sizes ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/camunda-cloud/zeebe/issues/8621"},"#8621"))),(0,r.kt)("h1",{id:"outtakes"},"Outtakes"),(0,r.kt)("p",null,"Interesting issues I run into when doing the chaos experiment, could be count as TIL events and mentioning them might help others."),(0,r.kt)("h2",{id:"message-pack-is-not-valid"},"Message pack is not valid"),(0,r.kt)("p",null,"When I first generated the JSON payload, it was an array on root level, which is not supported by Zeebe. "),(0,r.kt)("p",null,"I spent sometime to understand why I see no progress in processing. Taking a look at the gateway logs we can see:"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"\"Expected to handle gRPC request, but messagepack property was invalid: io.camunda.zeebe.msgpack.MsgpackPropertyException: Property 'variables' is invalid: Expected document to be a root level object, but was 'ARRAY'\"")),(0,r.kt)("p",null,"On the client side (if the logging is turned on, starter needs info logging) we see:"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"INVALID_ARGUMENT: Property 'variables' is invalid: Expected document to be a root level object, but was 'ARRAY'")),(0,r.kt)("h2",{id:"configure-the-starter-payload"},"Configure the Starter payload"),(0,r.kt)("p",null,"In order to use different JSON payload for the starter we support a configuration on the starter application (",(0,r.kt)("inlineCode",{parentName:"p"},"-Dapp.starter.payloadPath"),"). I had a lot of ",(0,r.kt)("em",{parentName:"p"},'"fun"')," to find out the right syntax:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'-Dapp.starter.payloadPath="bpmn/small_payload.json" - ',(0,r.kt)("em",{parentName:"li"},"DOESN'T WORK")),(0,r.kt)("li",{parentName:"ul"},'-Dapp.starter.payloadPath="/bpmn/small_payload.json" - ',(0,r.kt)("em",{parentName:"li"},"DOESN'T WORK")),(0,r.kt)("li",{parentName:"ul"},"-Dapp.starter.payloadPath=/bpmn/small_payload.json - ",(0,r.kt)("em",{parentName:"li"},"DOESN'T WORK")),(0,r.kt)("li",{parentName:"ul"},"-Dapp.starter.payloadPath=bpmn/big_payload.json - ",(0,r.kt)("em",{parentName:"li"},"WORKS"))),(0,r.kt)("p",null,"So be aware don't use ",(0,r.kt)("inlineCode",{parentName:"p"},'"')," and no ",(0,r.kt)("inlineCode",{parentName:"p"},"/")," in front, otherwise you might get a ",(0,r.kt)("inlineCode",{parentName:"p"},'java.io.FileNotFoundException: "bpmn/small_payload.json" (No such file or directory)')," in your starter deployment and wonder why you see no progress."))}h.isMDXComponent=!0},4607:function(e,t,a){t.Z=a.p+"assets/images/base-general-6f2d98f4e1437e7d6ccaeb9106e1ad4e.png"},3181:function(e,t,a){t.Z=a.p+"assets/images/big-payload-increase-res-ccf66088a1363c2c7bbf3d10cd430c9e.png"},3506:function(e,t,a){t.Z=a.p+"assets/images/big-payload-starter-gw-restarts-18e517c14c161d58190bd23b618b87cf.png"},1112:function(e,t,a){t.Z=a.p+"assets/images/cc-general-bd2b9235be276f75cd8c485229bf9b44.png"},83:function(e,t,a){t.Z=a.p+"assets/images/console-healthy-23ed317bbe9dc53934403731fb72b55c.png"},1924:function(e,t,a){t.Z=a.p+"assets/images/failed-operate-6cc764529ec07f8c422fb62e53289a02.png"},5082:function(e,t,a){t.Z=a.p+"assets/images/small-payload-7275166d05a6c32e2a0e7d86125f3c44.png"}}]);