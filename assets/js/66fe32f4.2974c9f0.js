"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[1769],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return h}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(n),h=r,d=m["".concat(l,".").concat(h)]||m[h]||u[h]||o;return n?a.createElement(d,i(i({ref:t},c),{},{components:n})):a.createElement(d,i({ref:t},c))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},6632:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return l},default:function(){return h},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return u}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),i=["components"],s={layout:"posts",title:"Gateway memory consumption",date:new Date("2020-10-20T00:00:00.000Z"),categories:["chaos_experiment","gateway","resources"],authors:"zell"},l="Chaos Day Summary",p={permalink:"/zeebe-chaos/2020/10/27/standalone-gw-memory",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-10-27-standalone-gw-memory/index.md",source:"@site/blog/2020-10-27-standalone-gw-memory/index.md",title:"Gateway memory consumption",description:"In the last weeks I check multiple benchmarks and clusters in incidents. Often I had the feeling that the memory consumption from the gateway is not ideal",date:"2020-10-20T00:00:00.000Z",formattedDate:"October 20, 2020",tags:[],readingTime:3.775,truncated:!0,authors:[{name:"Christopher Zell",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],prevItem:{title:"Non-graceful Shutdown Broker",permalink:"/zeebe-chaos/2020/10/20/non-graceful-shutdown"},nextItem:{title:"Multiple Leader Changes",permalink:"/zeebe-chaos/2020/10/13/multiple-leader-changes"}},c={authorsImageUrls:[void 0]},u=[{value:"Chaos experiment",id:"chaos-experiment",children:[{value:"Expected",id:"expected",children:[]},{value:"Actual",id:"actual",children:[]},{value:"Conclusion",id:"conclusion",children:[]},{value:"Other Observations",id:"other-observations",children:[]}]},{value:"New Issues",id:"new-issues",children:[]},{value:"Participants",id:"participants",children:[]}],m={toc:u};function h(e){var t=e.components,s=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},m,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"In the last weeks I check multiple benchmarks and clusters in incidents. Often I had the feeling that the memory consumption from the gateway is not ideal\nor that there is a memory leak. I wanted to experiment regarding this memory consumptions. Since we saw in investigating ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe/issues/5641"},"https://github.com/zeebe-io/zeebe/issues/5641")," a high memory spike\nwhen the gateway was not able to talk to other nodes I suspected that here might be some bugs hiding"),(0,o.kt)("h2",{id:"chaos-experiment"},"Chaos experiment"),(0,o.kt)("p",null,"We will run the Standalone gateway without the brokers and put load on it. "),(0,o.kt)("h3",{id:"expected"},"Expected"),(0,o.kt)("p",null,"All requests are rejected by the gateway and the memory doesn't grow infinitly on a steady load. The memory consumption should be stable at some point."),(0,o.kt)("h3",{id:"actual"},"Actual"),(0,o.kt)("p",null,"First I run the standalone gateway without any load. It seems that benchmarks without brokers are not shown in our dashboard namespaces. I fixed that for my experiment in my local grafana session. The issue was that we search for a ",(0,o.kt)("inlineCode",{parentName:"p"},"atomix_role")," metric to get the related namespaces. This metrics is only published on broker side."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(2061).Z})),(0,o.kt)("p",null,"We can see that even if there is no load the memory is already growing."),(0,o.kt)("p",null,"Putting more load on it showed that it doesn't drastically increase the memory consumption, but still it was growing."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(2742).Z})),(0,o.kt)("p",null,"I think the issue here is that we currently have no limits set for the gateway, which means it will use as many as it can. There is also no pressure for the GC to run or reclaim memory.\nWe probably want to limit it at somepoint. I created an issue for it ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe/issues/5699"},"#5699")," In order to find out whether we have a memory leak I used a profiler."),(0,o.kt)("p",null,"I restarted the experiment with new settings:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"# JavaOpts:\n# DEFAULTS\nJavaOpts: >-\n  # other options\n  -Djava.rmi.server.hostname=127.0.0.1\n  -Dcom.sun.management.jmxremote.port=9010\n  -Dcom.sun.management.jmxremote.rmi.port=9010\n  -Dcom.sun.management.jmxremote.authenticate=false\n  -Dcom.sun.management.jmxremote.ssl=false\n  -Dcom.sun.management.jmxremote.local.only=false\n")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"This will open a remote, unauthenticated, plaintext JMX connection - do not use this configuration in production!\nSee ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe/blob/develop/benchmarks/docs/debug/README.md#remote_jmx"},"https://github.com/zeebe-io/zeebe/blob/develop/benchmarks/docs/debug/README.md#remote_jmx"))),(0,o.kt)("p",null,"After I added a port forwarding I was able to open an JMX connection with Java Mission Control."),(0,o.kt)("h3",{id:"conclusion"},"Conclusion"),(0,o.kt)("p",null,"I profiled the gateway with and without load but haven't found any memory leak so far."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(6328).Z})),(0,o.kt)("p",null,"Also with VisualVM and triggering multiple GC's I was not able to spot any thing problematic."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(2241).Z})),(0,o.kt)("p",null,"In order to avoid that it uses too much memory and the memory continously grows we should set a limit for the Gateway (",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe/issues/5699"},"#5699"),")."),(0,o.kt)("h3",{id:"other-observations"},"Other Observations"),(0,o.kt)("h4",{id:"serialgc-usage"},"SerialGC usage"),(0,o.kt)("p",null,"Java Mission Control reported as an error that on a multi-core machine the serial garabage collector was used.\nIf we check the JVM properties we can see that as well."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(3688).Z})),(0,o.kt)("p",null,'This is weird because we don\'t set any GC in our benchmarks, so I would suspect the G1 is used with Java 11. Unfortunately this depends on the available resources which are "detected" by the JVM.\nRelated to that ',(0,o.kt)("a",{parentName:"p",href:"https://stackoverflow.com/questions/52474162/why-is-serialgc-chosen-over-g1gc"},"https://stackoverflow.com/questions/52474162/why-is-serialgc-chosen-over-g1gc"),"\nI think we should investigate that further, because we can see in Java mission control that we have GC pauses up to 8 seconds! I created a new issue for it ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe/issues/5700"},"#5700"),"."),(0,o.kt)("h4",{id:"unexpected-responses"},"Unexpected responses"),(0,o.kt)("p",null,"When we start the ",(0,o.kt)("inlineCode",{parentName:"p"},"starters")," they will first try to deploy a workflow model and loop in this state until they succeed.\nIn the metrics we can see that the responses to the deployment commands are ",(0,o.kt)("inlineCode",{parentName:"p"},"NOT_FOUND")," instead of ",(0,o.kt)("inlineCode",{parentName:"p"},"PARTITION_NOT_AVAILABLE"),", which I would expect."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(860).Z})),(0,o.kt)("p",null,"We can also see that in the log of the starter:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"11:01:18.035 [main] WARN  io.zeebe.Starter - Failed to deploy workflow, retrying\nio.zeebe.client.api.command.ClientStatusException: Expected to execute command, but this command refers to an element that doesn't exist.\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[app.jar:0.24.2]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[app.jar:0.24.2]\n    at io.zeebe.Starter.deployWorkflow(Starter.java:128) [app.jar:0.24.2]\n    at io.zeebe.Starter.run(Starter.java:55) [app.jar:0.24.2]\n    at io.zeebe.App.createApp(App.java:50) [app.jar:0.24.2]\n    at io.zeebe.Starter.main(Starter.java:142) [app.jar:0.24.2]\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Expected to execute command, but this command refers to an element that doesn't exist.\n    at java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]\n    at java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[app.jar:0.24.2]\n    ... 4 more\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Expected to execute command, but this command refers to an element that doesn't exist.\n    at io.grpc.Status.asRuntimeException(Status.java:533) ~[app.jar:0.24.2]\n    at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:460) ~[app.jar:0.24.2]\n    at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[app.jar:0.24.2]\n    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[app.jar:0.24.2]\n    at me.dinowernli.grpc.prometheus.MonitoringClientCallListener.onClose(MonitoringClientCallListener.java:50) ~[app.jar:0.24.2]\n    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:426) ~[app.jar:0.24.2]\n    at io.grpc.internal.ClientCallImpl.access$500(ClientCallImpl.java:66) ~[app.jar:0.24.2]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:689) ~[app.jar:0.24.2]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$900(ClientCallImpl.java:577) ~[app.jar:0.24.2]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:751) ~[app.jar:0.24.2]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:740) ~[app.jar:0.24.2]\n    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[app.jar:0.24.2]\n    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[app.jar :0.24.2]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\n    at java.lang.Thread.run(Unknown Source) ~[?:?]\n")),(0,o.kt)("p",null,"This doesn't make any sense. I created a new issue for it ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe/issues/5702"},"#5702")),(0,o.kt)("h2",{id:"new-issues"},"New Issues"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Limit Gateway ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/zeebe-io/zeebe/issues/5699"},"https://github.com/zeebe-io/zeebe/issues/5699")),(0,o.kt)("li",{parentName:"ul"},"SerialGC usage ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/zeebe-io/zeebe/issues/5700"},"https://github.com/zeebe-io/zeebe/issues/5700")),(0,o.kt)("li",{parentName:"ul"},"Wrong error response on deployment command ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/zeebe-io/zeebe/issues/5702"},"https://github.com/zeebe-io/zeebe/issues/5702"))),(0,o.kt)("h2",{id:"participants"},"Participants"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"@zelldon")))}h.isMDXComponent=!0},3688:function(e,t,n){t.Z=n.p+"assets/images/gc-settings-f4dcdc35aa83cff6c3e04b8236d7f227.png"},2742:function(e,t,n){t.Z=n.p+"assets/images/memory-gw-no-broker-high-load-79c5d9ec96d30db64a3e527e368deb57.png"},2061:function(e,t,n){t.Z=n.p+"assets/images/memory-gw-no-broker-no-load-573bc03afb7f975be6703ed7d306354a.png"},6328:function(e,t,n){t.Z=n.p+"assets/images/result-e55c32125c64c4cb680c4df77b690dbd.png"},860:function(e,t,n){t.Z=n.p+"assets/images/unexepcted-result-e571536d49a097f1e4feedcd0a10a61e.png"},2241:function(e,t,n){t.Z=n.p+"assets/images/visualvm-2a1bba44385aa57d3754764946e36e37.png"}}]);