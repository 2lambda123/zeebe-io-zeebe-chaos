"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[3556],{3905:(e,t,n)=>{n.d(t,{Zo:()=>l,kt:()=>u});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},l=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),h=p(n),u=o,m=h["".concat(s,".").concat(u)]||h[u]||d[u]||i;return n?a.createElement(m,r(r({ref:t},l),{},{components:n})):a.createElement(m,r({ref:t},l))}));function u(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=h;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:o,r[1]=c;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},526:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>c,toc:()=>p});var a=n(7462),o=(n(7294),n(3905));const i={layout:"posts",title:"Camunda Cloud network partition",date:new Date("2021-03-23T00:00:00.000Z"),categories:["chaos_experiment","camunda_cloud","network"],authors:"zell"},r="Chaos Day Summary",c={permalink:"/zeebe-chaos/2021/03/23/camunda-cloud-network-partition",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-03-23-camunda-cloud-network-partition/index.md",source:"@site/blog/2021-03-23-camunda-cloud-network-partition/index.md",title:"Camunda Cloud network partition",description:"This time Deepthi was joining me on my regular Chaos Day.",date:"2021-03-23T00:00:00.000Z",formattedDate:"March 23, 2021",tags:[],readingTime:7.18,truncated:!0,authors:[{name:"Christopher Zell",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],prevItem:{title:"Set file immutable",permalink:"/zeebe-chaos/2021/03/30/set-file-immutable"},nextItem:{title:"Fault-tolerant processing of process instances",permalink:"/zeebe-chaos/2021/03/09/cont-workflow-instance"}},s={authorsImageUrls:[void 0]},p=[{value:"Chaos Experiment",id:"chaos-experiment",children:[{value:"Enhancement",id:"enhancement",children:[]},{value:"Verification",id:"verification",children:[]},{value:"Testbench",id:"testbench",children:[]}]}],l={toc:p};function d(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,a.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"This time ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/deepthidevaki"},"Deepthi")," was joining me on my regular Chaos Day. \ud83c\udf89"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"/zeebe-chaos/2021/02/23/automate-deployments-dist"},"In the second last chaos day")," I created an automated chaos experiment, which verifies that the deployments are distributed after a network partition. Later it turned out that this doesn't work for camunda cloud, only for our helm setup. ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe-cluster-testbench/issues/237"},"The issue")," was that on our camunda cloud zeebe clusters we had no ",(0,o.kt)("a",{parentName:"p",href:"https://man7.org/linux/man-pages/man7/capabilities.7.html"},"NET_ADMIN")," capability to create ip routes (used for the network partitions). After discussing with our SRE's they proposed a good way to overcome this. On running chaos experiments, which are network related, we will patch our target cluster to add this capability. This means we don't need to add such functionality in camunda cloud and the related zeebe operate/controller. Big thanks to ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hisImminence"},"Immi")," and ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/Faffnir"},"David")," for providing this fix."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"TL;DR;")),(0,o.kt)("p",null,"We were able to enhance the deployment distribution experiment and run it in the camunda cloud via testbench. We have enabled the experiment for Production M and L cluster plans. We had to adjust the rights for the testbench service account to make this work."),(0,o.kt)("h2",{id:"chaos-experiment"},"Chaos Experiment"),(0,o.kt)("p",null,"We already had a ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/helm/deployment-distribution/experiment.json"},"prepared chaos experiment"),", but we needed to enhance that. Deepthi was so kind to create ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe-chaos/pull/50"},"PR")," for that."),(0,o.kt)("h3",{id:"enhancement"},"Enhancement"),(0,o.kt)("p",null,"The changes contain a new step before creating the network partition on the deployment distribution experiment, see ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/camunda-cloud/production-l/deployment-distribution/experiment.json#L25-L35"},"here"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'        {\n            "type": "action",\n            "name": "Enable net_admin capabilities",\n            "provider": {\n                "type": "process",\n                "path": "apply_net_admin.sh"\n            },\n            "pauses": {\n                "after": 180\n            }\n        }\n')),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"apply_net_admin.sh")," contains the following code:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},'\n#!/bin/bash\nset -euo pipefail\n\nscriptPath=$( cd "$(dirname "${BASH_SOURCE[0]}")" ; pwd -P )\nsource utils.sh\n\nnamespace=$(getNamespace)\n\nCLUSTERID=${namespace%-zeebe}\n\nkubectl patch zb "$CLUSTERID" --type merge --patch=\'{"spec":{"controller":{"reconcileDisabled":true}}}\'\nkubectl patch statefulset zeebe -n "$namespace" --patch "$(cat $scriptPath/net_admin_patch.yaml)"\n')),(0,o.kt)("p",null,"It disables reconciliation for the target zeebe cluster and applies the following patch, which adds the ",(0,o.kt)("inlineCode",{parentName:"p"},"NET_ADMIN")," capability:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  template:\n    spec:\n      containers:\n        - name: "zeebe"\n          securityContext:\n            capabilities:\n              add:\n                - "NET_ADMIN"\n')),(0,o.kt)("p",null,"Big thanks to ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hisImminence"},"Immi")," and ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/Faffnir"},"David")," for providing this fix."),(0,o.kt)("p",null,"After we applied the patch, we need to make sure that the all pods are restarted and have the requested change. This is the reason we wait after the action for some minutes. This was the easiest way for us to think of, but ideally we find a better way here to make sure the patch was applied and we can continue."),(0,o.kt)("h3",{id:"verification"},"Verification"),(0,o.kt)("p",null,"We run this experiment with a Production L cluster (v1.0.0-alpha2) and it succeeded. This is quite nice, because this also contains already the rewritten ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/camunda-cloud/zeebe/issues/6173"},"deployment distribution")," logic."),(0,o.kt)("p",null,"To verify whether the experiment really does something we checked the metrics and the logs. In the metrics we were not really able to tell that there was a network partition going on. There were no heart beats missing. The reason for that was that in the experiment the Leader for partition 3 (Node 1) and Leader for partition 1 (Node 0) have been disconnected. If we check the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/docs/scripts/partitionDistribution.sh"},"partition distribution")," we can see that they have no partitions in common. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"$ ./partitionDistribution.sh 6 8 3\nDistribution:\nP\\N|    N 0|    N 1|    N 2|    N 3|    N 4|    N 5\nP 0|    L  |    F  |    F  |    -  |    -  |    -  \nP 1|    -  |    L  |    F  |    F  |    -  |    -  \nP 2|    -  |    -  |    L  |    F  |    F  |    -  \nP 3|    -  |    -  |    -  |    L  |    F  |    F  \nP 4|    F  |    -  |    -  |    -  |    L  |    F  \nP 5|    F  |    F  |    -  |    -  |    -  |    L  \nP 6|    L  |    F  |    F  |    -  |    -  |    -  \nP 7|    -  |    L  |    F  |    F  |    -  |    -  \n\nPartitions per Node:\nN 0: 4\nN 1: 5\nN 2: 5\nN 3: 4\nN 4: 3\nN 5: 3\n")),(0,o.kt)("p",null,"Fortunately we saw in the logs that the Node 1, was retrying to send the deployments and at some point it succeeds."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"2021-03-23 13:11:54.163 CET\nFailed to receive deployment response for partitions [2, 4, 7, 8] (topic 'deployment-response-2251799813685304'). Retrying\n2021-03-23 13:11:54.164 CET\nPushed deployment 2251799813685304 to all partitions.\n2021-03-23 13:11:54.164 CET\nDeployment CREATE command for deployment 2251799813685304 was written on partition 2\n2021-03-23 13:11:54.164 CET\nDeployment CREATE command for deployment 2251799813685304 was written on partition 4\n2021-03-23 13:11:54.164 CET\nDeployment CREATE command for deployment 2251799813685304 was written on partition 8\n2021-03-23 13:11:54.165 CET\nDeployment CREATE command for deployment 2251799813685304 was written on partition 7\n2021-03-23 13:11:54.175 CET\nDeployment 2251799813685304 distributed to all partitions successfully.\n2021-03-23 13:11:54.763 CET\nFailed to receive deployment response for partitions [2, 4, 7, 8] (topic 'deployment-response-2251799813685306'). Retrying\n2021-03-23 13:11:54.764 CET\nPushed deployment 2251799813685306 to all partitions.\n2021-03-23 13:11:54.764 CET\nDeployment CREATE command for deployment 2251799813685306 was written on partition 4\n2021-03-23 13:11:54.764 CET\nDeployment CREATE command for deployment 2251799813685306 was written on partition 2\n2021-03-23 13:11:54.765 CET\nDeployment CREATE command for deployment 2251799813685306 was written on partition 8\n2021-03-23 13:11:54.765 CET\nDeployment CREATE command for deployment 2251799813685306 was written on partition 7\n2021-03-23 13:11:54.773 CET\nDeployment 2251799813685306 distributed to all partitions successfully.\n")),(0,o.kt)("h3",{id:"testbench"},"Testbench"),(0,o.kt)("p",null,"After the experiment has succeeded and we had verified the execution we run this again on testbench."),(0,o.kt)("p",null,"We saw a non completing chaos worker after checking the chaostoolkit logs, we saw that it was still in the phase of disconnecting the nodes, which was the same issue as before ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe-cluster-testbench/issues/237"},"#237"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"[2021-03-23 12:40:24 INFO] [activity:161] Action: Enable net_admin capabilities\n[2021-03-23 12:40:24 DEBUG] [process:53] Running: ['/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/apply_net_admin.sh']\n[2021-03-23 12:40:24 WARNING] [process:66] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\n[2021-03-23 12:40:24 DEBUG] [__init__:142] Data encoding detected as 'ascii' with a confidence of 1.0\n[2021-03-23 12:40:24 DEBUG] [activity:180]   => succeeded with '{'status': 1, 'stdout': '', 'stderr': 'Error from server (Forbidden): zeebeclusters.cloud.camunda.io \"cc108db7-768c-45cc-a6c5-098dc28f260c\" is forbidden: User \"system:serviceaccount:zeebe-chaos:zeebe-chaos-sa\" cannot get resource \"zeebeclusters\" in API group \"cloud.camunda.io\" at the cluster scope\\n'}'\n[2021-03-23 12:40:24 INFO] [activity:198] Pausing after activity for 180s...\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on 'activity'\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on 'activity'\n[2021-03-23 12:43:24 INFO] [activity:161] Probe: All pods should be ready\n[2021-03-23 12:43:24 DEBUG] [process:53] Running: ['/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/verify-readiness.sh']\n[2021-03-23 12:43:24 DEBUG] [__init__:142] Data encoding detected as 'ascii' with a confidence of 1.0\n[2021-03-23 12:43:24 DEBUG] [__init__:142] Data encoding detected as 'ascii' with a confidence of 1.0\n[2021-03-23 12:43:24 DEBUG] [activity:180]   => succeeded with '{'status': 0, 'stdout': 'pod/zeebe-0 condition met\\npod/zeebe-1 condition met\\npod/zeebe-2 condition met\\npod/zeebe-3 condition met\\npod/zeebe-4 condition met\\npod/zeebe-5 condition met\\n', 'stderr': \"+ source utils.sh\\n++ CHAOS_SETUP=cloud\\n++ getNamespace\\n+++ kubectl config view --minify --output 'jsonpath={..namespace}'\\n++ namespace=cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\n++ echo cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\n+ namespace=cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\n+ '[' cloud == cloud ']'\\n+ kubectl wait --for=condition=Ready pod -l app.kubernetes.io/app=zeebe --timeout=900s -n cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\n\"}'\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on 'activity'\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on 'activity'\n[2021-03-23 12:43:24 INFO] [activity:161] Action: Create network partition between leaders\n[2021-03-23 12:43:24 DEBUG] [process:53] Running: ['/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/disconnect-leaders.sh']\n")),(0,o.kt)("p",null,"In the logs we also saw that the patch didn't worked as expected with the used service account, so we need to fix here the permission and after that it should hopefully work. \ud83e\udd1e"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},'\'Error from server (Forbidden): zeebeclusters.cloud.camunda.io "XXX" is forbidden: User "system:serviceaccount:zeebe-chaos:zeebe-chaos-sa" cannot get resource "zeebeclusters" in API group "cloud.camunda.io" at the cluster scope\\n\'\n')),(0,o.kt)("p",null,"After checking with Immi, we were sure that we need to adjust the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe-cluster-testbench/blob/develop/core/chaos-workers/deployment/service-account/zeebe-chaos-role.yaml#L9"},"serviceaccount roles"),". After changing the apiGroups to ",(0,o.kt)("inlineCode",{parentName:"p"},'["*"]')," the experiments are running in testbench and the patch can be applied. We can now see in the log the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"[2021-03-23 14:21:24 DEBUG] [process:53] Running: ['/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/apply_net_admin.sh']\n[2021-03-23 14:21:25 DEBUG] [__init__:142] Data encoding detected as 'ascii' with a confidence of 1.0\n[2021-03-23 14:21:25 DEBUG] [activity:180]   => succeeded with '{'status': 0, 'stdout': 'zeebecluster.cloud.camunda.io/cc108db7-768c-45cc-a6c5-098dc28f260c patched\\nstatefulset.apps/zeebe patched\\n', 'stderr': ''}'\n")),(0,o.kt)("p",null,"Thanks for participating ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/deepthidevaki"},"Deepthi"),"."),(0,o.kt)("h4",{id:"found-bugs"},"Found Bugs"),(0,o.kt)("h5",{id:"re-connecting-might-fail"},"Re-connecting might fail"),(0,o.kt)("p",null,"We realized during testing the experiment that the re-connecting might fail, because the pod can be rescheduled and then a ip route can't be delete since it no longer exist. ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/scripts/connect-leaders.sh#L45-L48"},"This is now fixed"),". We check for existence of the command ",(0,o.kt)("inlineCode",{parentName:"p"},"ip"),", if this doesn't exist we know the pod was restarted and we ignore it."),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Before:")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},'function connect() {\n toChangedPod="$1"\n targetIp="$2"\n\n # update to have access to ip\n kubectl exec -n "$namespace" "$toChangedPod" -- apt update\n kubectl exec -n "$namespace" "$toChangedPod" -- apt install -y iproute2\n kubectl exec "$toChangedPod" -n "$namespace" -- ip route del unreachable "$targetIp"\n\n}\n')),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"After:")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},'\nfunction connect() {\n toChangedPod="$1"\n targetIp="$2"\n\n if command -v ip\n then\n     kubectl exec "$toChangedPod" -n "$namespace" -- ip route del unreachable "$targetIp"\n fi\n}\n')))}d.isMDXComponent=!0}}]);