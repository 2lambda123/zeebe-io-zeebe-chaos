"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[1662],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>d});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(a),h=i,d=u["".concat(l,".").concat(h)]||u[h]||m[h]||r;return a?n.createElement(d,o(o({ref:t},p),{},{components:a})):n.createElement(d,o({ref:t},p))}));function d(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},1473:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var n=a(7462),i=(a(7294),a(3905));const r={layout:"posts",title:"Using Large Multi-Instance",date:new Date("2023-06-02T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},o="Chaos Day Summary",s={permalink:"/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-06-02-Using-Large-Multi-Instance/index.md",source:"@site/blog/2023-06-02-Using-Large-Multi-Instance/index.md",title:"Using Large Multi-Instance",description:"New day new chaos. In today's chaos day I want to pick up a topic, which had bothered people for long time. I created a chaos day three years ago around this topic as well.",date:"2023-06-02T00:00:00.000Z",formattedDate:"June 2, 2023",tags:[{label:"availability",permalink:"/zeebe-chaos/tags/availability"}],readingTime:5.77,hasTruncateMarker:!0,authors:[{name:"Christopher Zell",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],frontMatter:{layout:"posts",title:"Using Large Multi-Instance",date:"2023-06-02T00:00:00.000Z",categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},nextItem:{title:"Continuing SST Partitioning toggle",permalink:"/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle"}},l={authorsImageUrls:[void 0]},c=[{value:"Chaos Experiment",id:"chaos-experiment",level:2},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"Starting small (20k)",id:"starting-small-20k",level:4},{value:"Increase collection (200k)",id:"increase-collection-200k",level:4},{value:"Make it really big (2 million)",id:"make-it-really-big-2-million",level:4},{value:"Results",id:"results",level:3},{value:"Found Bugs",id:"found-bugs",level:2}],p={toc:c},u="wrapper";function m(e){let{components:t,...r}=e;return(0,i.kt)(u,(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"New day new chaos. \ud83d\udc80 In today's chaos day I want to pick up a topic, which had bothered people for long time. I created a ",(0,i.kt)("a",{parentName:"p",href:"https://zeebe-io.github.io/zeebe-chaos/2020/07/16/big-multi-instance/"},"chaos day three years ago")," around this topic as well. "),(0,i.kt)("p",null,"Today, we experiment with large multi-instances again. In the recent patch release ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/camunda/zeebe/releases/tag/8.2.5"},"8.2.5")," we fixed an issue with spawning larger multi instances. Previously if you have created a process instance with a large multi-instance it was likely that this caused to blacklist the process instance, since the multi-instance spawning ran into ",(0,i.kt)("inlineCode",{parentName:"p"},"maxMessageSize")," limitations. "),(0,i.kt)("p",null,"This means the process instance was stuck and was no longer executable. In Operate this was not shown and caused a lot of friction or confusion to users. With the recent fix, Zeebe should chunk even large collections into smaller batches to spawn/execute the multi-instance without any issues."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"TL;DR;")," We were able to see that even large multi-instances can be executed now. \u2705 At some point, we experienced performance regressions (during creating new multi-instance elements) but the execution of the process instance doesn't fail anymore. One problem at a time, we will likely investigate further to improve the performance of such a use case."),(0,i.kt)("p",null,"When we reached the ",(0,i.kt)("inlineCode",{parentName:"p"},"maxMessageSize")," we got a rejection, if the input collection is too large we see some weird unexpected errors from NGINX."),(0,i.kt)("h2",{id:"chaos-experiment"},"Chaos Experiment"),(0,i.kt)("p",null,"We do regularly game days in Camunda, and for such we also create projects to make incidents, etc. reproducible. In today's chaos day, I will reuse some code created by ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/saig0"},"Philipp Ossler"),", thanks for that \ud83d\ude47\u200d\u2642\ufe0f Since we mimic in such game days customers, the process is a bit more complex than necessary for such chaos day, but I will keep it like that."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"order-process",src:a(3238).Z,width:"3213",height:"1152"})),(0,i.kt)("p",null,"The input collection ",(0,i.kt)("inlineCode",{parentName:"p"},"items"),", which is used in the multi-instance is generated via:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'    // input size\n    final var items = IntStream.range(0, size).mapToObj(i -> Map.ofEntries(\n        entry("id", i)\n    )).toList();\n')),(0,i.kt)("p",null,"In the following experiment, we will play around with the ",(0,i.kt)("inlineCode",{parentName:"p"},"size")," value."),(0,i.kt)("p",null,"For the experiment, we will use a Camunda 8 SaaS cluster with the generation ",(0,i.kt)("inlineCode",{parentName:"p"},"Zeebe 8.2.5")," (G3-S)."),(0,i.kt)("h3",{id:"expected"},"Expected"),(0,i.kt)("p",null,"When creating a process instance with a large collection, we expect based on the recent bug fix that the multi-instance creation is batched and created without issues. "),(0,i.kt)("p",null,"One limiting factor might be the ",(0,i.kt)("inlineCode",{parentName:"p"},"maxMessageSize")," with regard to the input collection, but in this case, I would expect that the creation of the process instance is already rejected before."),(0,i.kt)("h3",{id:"actual"},"Actual"),(0,i.kt)("p",null,"Between the following experiments, I always recreated the clusters, to reduce the blast radius and better understand and isolate the impact. "),(0,i.kt)("h4",{id:"starting-small-20k"},"Starting small (20k)"),(0,i.kt)("p",null,"In previous versions, the multi-instance creation failed already quite early. For example in the game day reproducer project, we had a collection defined with ",(0,i.kt)("inlineCode",{parentName:"p"},"20.000")," items, which we are now reusing for the start."),(0,i.kt)("p",null,"The creation of the process instance worked without any issues. We can observe in Operate the incremental creation of sub-process instances, which is great."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"incremental-creation-20k",src:a(5146).Z,width:"1757",height:"868"})),(0,i.kt)("p",null,'We can see in the metrics that batch processing is limited by only 2-4 commands in a batch. That is an interesting fact that might explain why it takes a while until all instances of the multi-instance sub-process are created. We can even see rollbacks during batch processing, visible in the "Number of batch processing retries" panel.'),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"processing-metrics-20k",src:a(3113).Z,width:"1894",height:"755"})),(0,i.kt)("p",null,"The processing queue seems to increase dramatically."),(0,i.kt)("p",null,"After a while, we can see that all 20k instances are created without any bigger issues. \ud83d\ude80"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"complete-20k",src:a(5069).Z,width:"1912",height:"834"})),(0,i.kt)("p",null,"It took around 10 minutes. Taking a look at the metrics again we see that in between big command batches have been created/processed, which allowed us to reduce the processing queue."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"processing-metrics-20k-pt2",src:a(350).Z,width:"1896",height:"762"})),(0,i.kt)("p",null,"In between the backpressure was quite high, but after the creation of all instances, the cluster is in a healthy state again. The creation of such multi-instance worked \u2705"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"general-metrics-20k",src:a(1187).Z,width:"1893",height:"940"})),(0,i.kt)("h4",{id:"increase-collection-200k"},"Increase collection (200k)"),(0,i.kt)("p",null,"Again, the creation of such a process instance was not a problem itself. We can observe the creation of the sub-process instances (multi-instance) in Operate, which happens incrementally."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"incremental-creation-200k",src:a(6642).Z,width:"1905",height:"871"})),(0,i.kt)("p",null,"It takes ages until the instances are created (After 3h ~66k instances are created). Again we see here small chunks of batches, and there are also rollbacks during batch processing."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"processing-metrics-200k",src:a(5471).Z,width:"1881",height:"753"})),(0,i.kt)("p",null,"The processing of that partitions is in this case blocked by the multi-instance creation, we can see that on the 100% back pressure. \u274c"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"general-metrics-200k",src:a(3750).Z,width:"1883",height:"868"})),(0,i.kt)("p",null,"Even after one hour, not all instances are created (not even 20k), it takes longer than before the creation of 20.000 instances."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"incremental-creation-200k-part2",src:a(2722).Z,width:"2251",height:"931"})),(0,i.kt)("h4",{id:"make-it-really-big-2-million"},"Make it really big (2 million)"),(0,i.kt)("p",null,"To escalate this even more I increase the input collection again by a factor of 10 to 2 million."),(0,i.kt)("p",null,"After creation, I see as a response the following log message in my log:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Failed to create process instance of 'order-process'\n\nio.camunda.zeebe.client.api.command.ClientStatusException: HTTP status code 502\ninvalid content-type: text/html\nheaders: Metadata(:status=502,date=Fri, 02 Jun 2023 11:44:57 GMT,content-type=text/html,strict-transport-security=max-age=63072000; includeSubDomains,content-length=150)\nDATA-----------------------------\n<html>\n<head><title>502 Bad Gateway</title></head>\n<body>\n<center><h1>502 Bad Gateway</h1></center>\n<hr><center>nginx</center>\n</body>\n</html>\n\n    at io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-8.0.5.jar:8.0.5]\n    at io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-8.0.5.jar:8.0.5]\n    at io.camunda.cloud.gameday.ProcessApplication.createProcessInstance(ProcessApplication.java:90) ~[classes/:na]\n    at io.camunda.cloud.gameday.ProcessApplication.createInstanceOfProcess(ProcessApplication.java:71) ~[classes/:na]\n    at io.camunda.cloud.gameday.ProcessApplication.run(ProcessApplication.java:58) ~[classes/:na]\n    at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:791) ~[spring-boot-2.5.2.jar:2.5.2]\n    at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:775) ~[spring-boot-2.5.2.jar:2.5.2]\n    at org.springframework.boot.SpringApplication.run(SpringApplication.java:345) ~[spring-boot-2.5.2.jar:2.5.2]\n    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1343) ~[spring-boot-2.5.2.jar:2.5.2]\n    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1332) ~[spring-boot-2.5.2.jar:2.5.2]\n    at io.camunda.cloud.gameday.ProcessApplication.main(ProcessApplication.java:46) ~[classes/:na]\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: HTTP status code 502\ninvalid content-type: text/html\nheaders: Metadata(:status=502,date=Fri, 02 Jun 2023 11:44:57 GMT,content-type=text/html,strict-transport-security=max-age=63072000; includeSubDomains,content-length=150)\nDATA-----------------------------\n<html>\n<head><title>502 Bad Gateway</title></head>\n<body>\n<center><h1>502 Bad Gateway</h1></center>\n<hr><center>nginx</center>\n</body>\n</html>\n\n    at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) ~[na:na]\n    at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) ~[na:na]\n    at io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-8.0.5.jar:8.0.5]\n    ... 9 common frames omitted\n")),(0,i.kt)("p",null,"I tried to incrementally decrease the input collection until it is working again, when reaching 250k I finally see a better understandable error."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"2023-06-02 13:53:51.485 ERROR 29870 --- [           main] i.c.cloud.gameday.ProcessApplication     : Failed to create process instance of 'order-process'\n\nio.camunda.zeebe.client.api.command.ClientStatusException: Command 'CREATE' rejected with code 'EXCEEDED_BATCH_RECORD_SIZE': \n    at io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-8.0.5.jar:8.0.5]\n    at io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-8.0.5.jar:8.0.5]\n    at io.camunda.cloud.gameday.ProcessApplication.createProcessInstance(ProcessApplication.java:90) ~[classes/:na]\n    at io.camunda.cloud.gameday.ProcessApplication.createInstanceOfProcess(ProcessApplication.java:71) ~[classes/:na]\n    at io.camunda.cloud.gameday.ProcessApplication.run(ProcessApplication.java:58) ~[classes/:na]\n    at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:791) ~[spring-boot-2.5.2.jar:2.5.2]\n    at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:775) ~[spring-boot-2.5.2.jar:2.5.2]\n    at org.springframework.boot.SpringApplication.run(SpringApplication.java:345) ~[spring-boot-2.5.2.jar:2.5.2]\n    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1343) ~[spring-boot-2.5.2.jar:2.5.2]\n    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1332) ~[spring-boot-2.5.2.jar:2.5.2]\n    at io.camunda.cloud.gameday.ProcessApplication.main(ProcessApplication.java:46) ~[classes/:na]\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNKNOWN: Command 'CREATE' rejected with code 'EXCEEDED_BATCH_RECORD_SIZE': \n    at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) ~[na:na]\n    at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) ~[na:na]\n    at io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-8.0.5.jar:8.0.5]\n    ... 9 common frames omitted\nCaused by: io.grpc.StatusRuntimeException: UNKNOWN: Command 'CREATE' rejected with code 'EXCEEDED_BATCH_RECORD_SIZE': \n    at io.grpc.Status.asRuntimeException(Status.java:535) ~[grpc-api-1.45.1.jar:1.45.1]\n    at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.45.1.jar:1.45.1]\n    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:562) ~[grpc-core-1.45.1.jar:1.45.1]\n    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.45.1.jar:1.45.1]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:743) ~[grpc-core-1.45.1.jar:1.45.1]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:722) ~[grpc-core-1.45.1.jar:1.45.1]\n    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.45.1.jar:1.45.1]\n    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) ~[grpc-core-1.45.1.jar:1.45.1]\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[na:na]\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[na:na]\n    at java.base/java.lang.Thread.run(Thread.java:833) ~[na:na]\n\n2023-06-02 13:53:51.485  INFO 29870 --- [           main] i.c.cloud.gameday.ProcessApplication     : Created process instances with large collection. [order-id: 'ba65b59b-1584-48bb-af05-3724ea15fac9']\n\n")),(0,i.kt)("h3",{id:"results"},"Results"),(0,i.kt)("p",null,"As we have seen above we are able now to create much larger multi instances than before, with some drawbacks in performance, which needs to be investigated further."),(0,i.kt)("p",null,"When reaching a certain limit (maxMessageSize) we get a described rejection by the broker, until we reach the limit of NGINX where the description is not that optimal. Here we can and should improve further."),(0,i.kt)("h2",{id:"found-bugs"},"Found Bugs"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"in a previous test I run into ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/camunda/zeebe/issues/12918"},"https://github.com/camunda/zeebe/issues/12918")),(0,i.kt)("li",{parentName:"ul"},"Related bug regarding the input collection ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/camunda/zeebe/issues/12873"},"https://github.com/camunda/zeebe/issues/12873"))))}m.isMDXComponent=!0},3750:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/200k-general-metrics-3975fc0c4c761c321194cd69598a8d11.png"},6642:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/200k-operate-inc-586ee3f6c124c92cc6700df1b6213950.png"},2722:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/200k-operate-inc2-8e393502149c7dc21706229b02979515.png"},5471:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/200k-processing-metrics-7a21b4741eec54581b30f819b05f3de2.png"},1187:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/20k-general-metrics-a525d7f1500dac0fd03d460b849e12e4.png"},5069:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/20k-operate-complete-31c5d2b848c51c7b1a96e8dc6f23b5d6.png"},5146:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/20k-operate-inc-b0e507b77799c7f950ff444259c8341f.png"},350:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/20k-processing-metrics-2-7976048beb0f6300cf5577d40f2c9f47.png"},3113:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/20k-processing-metrics-f3f057027ec451f9024d242df2d1bef6.png"},3238:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/order-process-0eb9e9dc5b698919f847deb859f67f2d.png"}}]);