"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[5921],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>u});var o=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=o.createContext({}),p=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return o.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(n),u=r,h=m["".concat(l,".").concat(u)]||m[u]||d[u]||a;return n?o.createElement(h,i(i({ref:t},c),{},{components:n})):o.createElement(h,i({ref:t},c))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,i=new Array(a);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<a;p++)i[p]=n[p];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},6869:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>p});var o=n(7462),r=(n(7294),n(3905));const a={layout:"posts",title:"Automating Deployment Distribution Chaos Experiment",date:new Date("2021-02-23T00:00:00.000Z"),categories:["chaos_experiment","broker","network","deployment"],tags:["tests"],authors:"zell"},i="Chaos Day Summary",s={permalink:"/zeebe-chaos/2021/02/23/automate-deployments-dist",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-02-23-automate-deployments-dist/index.md",source:"@site/blog/2021-02-23-automate-deployments-dist/index.md",title:"Automating Deployment Distribution Chaos Experiment",description:"This time I wanted to automate a chaos experiment via the ChaosToolkit, which I did on the last chaos day. For a recap check out the last chaos day summary.",date:"2021-02-23T00:00:00.000Z",formattedDate:"February 23, 2021",tags:[{label:"tests",permalink:"/zeebe-chaos/tags/tests"}],readingTime:6.855,truncated:!0,authors:[{name:"Christopher Zell",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],prevItem:{title:"Fault-tolerant processing of process instances",permalink:"/zeebe-chaos/2021/03/09/cont-workflow-instance"},nextItem:{title:"Deployment Distribution",permalink:"/zeebe-chaos/2021/01/26/deployments"}},l={authorsImageUrls:[void 0]},p=[{value:"Chaos Experiment",id:"chaos-experiment",children:[{value:"Expected",id:"expected",children:[]},{value:"Experiment Definition",id:"experiment-definition",children:[]},{value:"Outcome",id:"outcome",children:[]}]}],c={toc:p};function d(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,o.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"This time I wanted to automate a chaos experiment via the ",(0,r.kt)("a",{parentName:"p",href:"https://chaostoolkit.org/"},"ChaosToolkit"),", which I did on the last chaos day. For a recap check out the ",(0,r.kt)("a",{parentName:"p",href:"/zeebe-chaos/2021/01/26/deployments"},"last chaos day summary"),"."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"TL;DR;")),(0,r.kt)("p",null,"I was able to automate the deployment distribution chaos experiment successfully and deployed it on testbench for a ",(0,r.kt)("inlineCode",{parentName:"p"},"Production - M")," cluster plan."),(0,r.kt)("h2",{id:"chaos-experiment"},"Chaos Experiment"),(0,r.kt)("p",null,"For testing, I have run our normal setup of three nodes, three partitions and replication factor three.\nLater I wanted to automate the experiment against the production cluster plans."),(0,r.kt)("h3",{id:"expected"},"Expected"),(0,r.kt)("p",null,"We want to verify whether deployments are still distributed after a network partition, for that we will write the following chaos experiment:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Verify Steady State: All Pods are ready"),(0,r.kt)("li",{parentName:"ol"},"Introduce Chaos:",(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("em",{parentName:"li"},"Action:")," Disconnect two leaders (Leader of partition one and another leader)"),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("em",{parentName:"li"},"Action:")," Deploy multiple versions of a workflow"),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("em",{parentName:"li"},"Action:")," Connect two leaders again"),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("em",{parentName:"li"},"Probe"),": I can create workflow instance on all partitions with the last workflow version"))),(0,r.kt)("li",{parentName:"ol"},"Verify Steady State: All Pods are ready")),(0,r.kt)("h3",{id:"experiment-definition"},"Experiment Definition"),(0,r.kt)("p",null,"The experiment definition looks like the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "version": "0.1.0",\n    "title": "Zeebe deployment distribution",\n    "description": "Zeebe deployment distribution should be fault-tolerant. Zeebe should be able to handle network outages and fail-overs and distribute the deployments after partitions are available again.",\n    "contributions": {\n        "reliability": "high",\n        "availability": "high"\n    },\n    "steady-state-hypothesis": {\n        "title": "Zeebe is alive",\n        "probes": [\n            {\n                "name": "All pods should be ready",\n                "type": "probe",\n                "tolerance": 0,\n                "provider": {\n                    "type": "process",\n                    "path": "verify-readiness.sh",\n                    "timeout": 900\n                }\n            }\n        ]\n    },\n    "method": [\n        {\n            "type": "action",\n            "name": "Create network partition between leaders",\n            "provider": {\n                "type": "process",\n                "path": "disconnect-leaders.sh"\n            }\n        },\n        {\n            "type": "action",\n            "name": "Deploy different deployment versions.",\n            "provider": {\n                "type": "process",\n                "path": "deploy-different-versions.sh",\n                "arguments": ["Follower", "3"]\n            }\n        },\n        {\n            "type": "action",\n            "name": "Delete network partition",\n            "provider": {\n                "type": "process",\n                "path": "connect-leaders.sh"\n            }\n        },\n        {\n            "type": "probe",\n            "name": "Create workflow instance of latest version on partition one",\n            "tolerance": 0,\n            "provider": {\n                "type": "process",\n                "path": "start-instance-on-partition-with-version.sh",\n                "arguments": ["1", "10"],\n                "timeout": 900\n            }\n        },\n        {\n            "type": "probe",\n            "name": "Create workflow instance of latest version on partition two",\n            "tolerance": 0,\n            "provider": {\n                "type": "process",\n                "path": "start-instance-on-partition-with-version.sh",\n                "arguments": ["2", "10"],\n                "timeout": 900\n            }\n        },\n        {\n            "type": "probe",\n            "name": "Create workflow instance of latest version on partition three",\n            "tolerance": 0,\n            "provider": {\n                "type": "process",\n                "path": "start-instance-on-partition-with-version.sh",\n                "arguments": ["3", "10"],\n                "timeout": 900\n            }\n        }\n    ],\n    "rollbacks": []\n}\n')),(0,r.kt)("h4",{id:"create-network-partition-between-leaders"},"Create network partition between leaders"),(0,r.kt)("p",null,"We reuse a script which we introduce in earlier chaos days. It needed to be improved a bit, since we haven't handled clusters where one node leads multiple partitions."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'#!/bin/bash\nset -exuo pipefail\n\nsource utils.sh\n\npartition=1\nnamespace=$(getNamespace)\ngateway=$(getGateway)\n\n# determine leader for partition one\nindex=$(getIndexOfPodForPartitionInState "$partition" "LEADER")\nleader=$(getBroker "$index")\nleaderIp=$(kubectl get pod "$leader" -n "$namespace" --template="{{.status.podIP}}")\n\n# determine leader for partition three\nindex=$(getIndexOfPodForPartitionInState "3" "LEADER")\nleaderTwo=$(getBroker "$index")\nleaderTwoIp=$(kubectl get pod "$leaderTwo" -n "$namespace" --template="{{.status.podIP}}")\n\nif [ "$leaderTwo" == "$leader" ]\nthen\n  # determine leader for partition two\n  index=$(getIndexOfPodForPartitionInState "2" "LEADER")\n  leaderTwo=$(getBroker "$index")\n  leaderTwoIp=$(kubectl get pod "$leaderTwo" -n "$namespace" --template="{{.status.podIP}}")\n\n  if [ "$leaderTwo" == "$leader" ]\n  then\n    # We could try to kill the pod and hope that he is not able to become leader again,\n    # but there is a high chance that it is able to do so after restart. It can make our test fragile,\n    # especially if we want to connect again, which is the reason why we do nothing in that case.\n    exit\n  fi\nfi\n\n# To print the topology in the journal\nretryUntilSuccess kubectl exec "$gateway" -n "$namespace" -- zbctl status --insecure\n\n# we put all into one function because we need to make sure that even after preemption the \n# dependency is installed\nfunction disconnect() {\n toChangedPod="$1"\n targetIp="$2"\n\n # update to have access to ip\n kubectl exec -n "$namespace" "$toChangedPod" -- apt update\n kubectl exec -n "$namespace" "$toChangedPod" -- apt install -y iproute2\n kubectl exec "$toChangedPod" -n "$namespace" -- ip route add unreachable "$targetIp"\n\n}\n\nretryUntilSuccess disconnect "$leader" "$leaderTwoIp"\nretryUntilSuccess disconnect "$leaderTwo" "$leaderIp" \n')),(0,r.kt)("h4",{id:"delete-network-partition"},"Delete network partition"),(0,r.kt)("p",null,"Looks similar to the disconnect script."),(0,r.kt)("h5",{id:"deploy-different-deployment-versions"},"Deploy different deployment versions"),(0,r.kt)("p",null,"This script is interesting. My first approach was to have one workflow model, where I replace an comment via ",(0,r.kt)("inlineCode",{parentName:"p"},"sed")," before redeploying. Later I realized it is much easier to just have two versions of the same worfklow model in the repository and deploy them in an alternating manner. This reduced the dependecy of an extra tool (",(0,r.kt)("inlineCode",{parentName:"p"},"sed"),"), which might not be available everywhere or work differently on different linux distributions. "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'#!/bin/bash\n\nset -exuo pipefail\n\nscriptPath=$( cd "$(dirname "${BASH_SOURCE[0]}")" ; pwd -P )\nsource utils.sh\n\nnamespace=$(getNamespace)\npod=$(getGateway)\n\nbpmnPath="$scriptPath/../bpmn/"\n\n# we put both together in one function to retry both, because it might be that pod has been restarted\n# then the model is not longer on the node, which cause endless retries of deployments\nfunction deployModel() {\n  kubectl cp "$bpmnPath" "$pod:/tmp/" -n "$namespace"\n\n  for i in {1..5}\n  do\n    # the models differ in one line, the share the same name and process id\n    # if we deploy them after another it will create two different deployment versions\n    # the deploy command only compares the last applied deployment - so we can do that in a loop to cause\n    # multiple deployments\n    kubectl exec "$pod" -n "$namespace" -- sh -c "zbctl deploy /tmp/bpmn/multi-version/multiVersionModel.bpmn --insecure"\n    kubectl exec "$pod" -n "$namespace" -- sh -c "zbctl deploy /tmp/bpmn/multi-version/multiVersionModel_v2.bpmn --insecure"\n  done\n}\n\nretryUntilSuccess deployModel\n')),(0,r.kt)("p",null,"When running this script we deploy ",(0,r.kt)("inlineCode",{parentName:"p"},"10")," new versions of the workflow ",(0,r.kt)("inlineCode",{parentName:"p"},"multiVersion"),"."),(0,r.kt)("h4",{id:"create-workflow-instance-of-latest-version-on-partition-x"},"Create workflow instance of latest version on partition X"),(0,r.kt)("p",null,"The following script allows us to be sure that we can create a workflow instance on a specific partition with the given version of the ",(0,r.kt)("inlineCode",{parentName:"p"},"multiVersion")," workflow.\nThis means we can verify that on all partitions the last deployment version is deployed/distributed."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'#!/bin/bash\nset -xo pipefail\n\nif [ -z "$1" ]\nthen\n  echo "Please provide an required partition!"\n  exit 1\nfi\n\nif [ -z "$2" ]\nthen\n  echo "Please provide an required deployment version!"\n  exit 1\nfi\n\nsource utils.sh\n\nnamespace=$(getNamespace)\npod=$(getGateway)\n\nrequiredPartition=$1\nrequiredDeploymentVersion=$2\nprocessId="multiVersion"\n\n# we put all into one function because we need to make sure that even after preemption the\n# dependency are installed, which is in this case is the deployment\nfunction startInstancesOnPartition() {\n\n  partition=0\n  until [[ "$partition" -eq "$requiredPartition" ]]; do\n    workflowInstanceKey=$(kubectl exec "$pod" -n "$namespace" -- zbctl create instance "$processId" --version "$requiredDeploymentVersion" --insecure)\n    workflowInstanceKey=$(echo "$workflowInstanceKey" | jq \'.workflowInstanceKey\')\n    partition=$(( workflowInstanceKey >> 51 ))\n    echo "Started workflow with key $workflowInstanceKey, corresponding partition $partition"\n  done\n}\n\nretryUntilSuccess startInstancesOnPartition\n')),(0,r.kt)("h3",{id:"outcome"},"Outcome"),(0,r.kt)("p",null,"After running this experiment we get the following output, which shows us that the experiment ",(0,r.kt)("strong",{parentName:"p"},"SUCCEEDED"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"(chaostk) [zell camunda-cloud/ cluster: zeebe-cluster ns:zell-chaos]$ chaos run production-m/deployment-distribution/experiment.json \n[2021-02-23 14:15:06 INFO] Validating the experiment's syntax\n[2021-02-23 14:15:06 INFO] Experiment looks valid\n[2021-02-23 14:15:06 INFO] Running experiment: Zeebe deployment distribution\n[2021-02-23 14:15:06 INFO] Steady-state strategy: default\n[2021-02-23 14:15:06 INFO] Rollbacks strategy: default\n[2021-02-23 14:15:06 INFO] Steady state hypothesis: Zeebe is alive\n[2021-02-23 14:15:06 INFO] Probe: All pods should be ready\n[2021-02-23 14:15:07 INFO] Probe: Should be able to create workflow instances on partition 3\n[2021-02-23 14:15:10 INFO] Steady state hypothesis is met!\n[2021-02-23 14:15:10 INFO] Playing your experiment's method now...\n[2021-02-23 14:15:10 INFO] Action: Create network partition between leaders\n[2021-02-23 14:15:26 INFO] Action: Deploy thousand different deployment versions.\n[2021-02-23 14:15:31 INFO] Action: Delete network partition\n[2021-02-23 14:15:43 INFO] Probe: Create workflow instance of latest version on partition one\n[2021-02-23 14:15:43 INFO] Probe: Create workflow instance of latest version on partition two\n[2021-02-23 14:15:51 INFO] Probe: Create workflow instance of latest version on partition three\n[2021-02-23 14:15:52 INFO] Steady state hypothesis: Zeebe is alive\n[2021-02-23 14:15:52 INFO] Probe: All pods should be ready\n[2021-02-23 14:15:52 INFO] Probe: Should be able to create workflow instances on partition 3\n[2021-02-23 14:15:55 INFO] Steady state hypothesis is met!\n[2021-02-23 14:15:55 INFO] Let's rollback...\n[2021-02-23 14:15:55 INFO] No declared rollbacks, let's move on.\n[2021-02-23 14:15:55 INFO] Experiment ended with status: completed\n")),(0,r.kt)("h4",{id:"testbench"},"Testbench"),(0,r.kt)("p",null,"I executed the new experiment via zeebe testbench, to verify that this works with the ",(0,r.kt)("inlineCode",{parentName:"p"},"Production - M")," cluster plan and it was successful \ud83d\udcaa"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"operate",src:n(2021).Z})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{"results":["production-m/deployment-distribution/experiment.json completed successfully","production-m/follower-restart/experiment.json completed successfully","production-m/follower-terminate/experiment.json completed successfully","production-m/leader-restart/experiment.json completed successfully","production-m/leader-terminate/experiment.json completed successfully","production-m/msg-correlation/experiment.json completed successfully","production-m/multiple-leader-restart/experiment.json completed successfully","production-m/snapshot-corruption/experiment.json completed successfully","production-m/stress-cpu-on-broker/experiment.json completed successfully","production-m/worker-restart/experiment.json completed successfully"]}\n')))}d.isMDXComponent=!0},2021:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/operate-success-6cfd18e98918ee25187b83ca4b925442.png"}}]);