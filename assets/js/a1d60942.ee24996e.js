"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[7272],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return d}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(n),d=a,m=u["".concat(l,".").concat(d)]||u[d]||h[d]||i;return n?r.createElement(m,o(o({ref:t},p),{},{components:n})):r.createElement(m,o({ref:t},p))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},8172:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},assets:function(){return p},toc:function(){return h},default:function(){return d}});var r=n(7462),a=n(3366),i=(n(7294),n(3905)),o=["components"],s={layout:"posts",title:"Gateway Network Partition",date:new Date("2020-06-25T00:00:00.000Z"),categories:["chaos_experiment","gateway"],authors:"zell"},l="Chaos Day Summary",c={permalink:"/zeebe-chaos/2020/06/25/gateway-network-partition",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-06-25-gateway-network-partition/index.md",source:"@site/blog/2020-06-25-gateway-network-partition/index.md",title:"Gateway Network Partition",description:"* Documented failure cases for AsyncSnasphortDirector. Gave me some ideas where it might make sense to reinstall partition. Discussed a bit with @Deepthi",date:"2020-06-25T00:00:00.000Z",formattedDate:"June 25, 2020",tags:[],readingTime:2.34,truncated:!0,authors:[{name:"Christopher Zell",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],prevItem:{title:"Extract K8 resources from namespace",permalink:"/zeebe-chaos/2020/07/02/extract-k8-resources"},nextItem:{title:"Correlate Message after failover",permalink:"/zeebe-chaos/2020/06/18/correlate-message-after-failover"}},p={authorsImageUrls:[void 0]},h=[{value:"Chaos experiment:",id:"chaos-experiment",children:[{value:"Expected during the experiment:",id:"expected-during-the-experiment",children:[]},{value:"Observations:",id:"observations",children:[]}]},{value:"Participants",id:"participants",children:[]}],u={toc:h};function d(e){var t=e.components,s=(0,a.Z)(e,o);return(0,i.kt)("wrapper",(0,r.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Documented failure cases for AsyncSnasphortDirector. Gave me some ideas where it might make sense to reinstall partition. Discussed a bit with @Deepthi"),(0,i.kt)("li",{parentName:"ul"},"Still our automated chaos experiments are not running. I need some time for that, but I had no time for that today."),(0,i.kt)("li",{parentName:"ul"},"Run a chaos experiment together with @pihme, where we do a network partition with the gateway.")),(0,i.kt)("h2",{id:"chaos-experiment"},"Chaos experiment:"),(0,i.kt)("p",null,"Actually we already have an network partition experiment with the standalone Gateway, where we completely isolate the gateway and take a look whether it comes back after the network partition. Today we wanted to explore how it behaves when only one node and the gateway has a network partition, so Broker 0 and Gateway can't talk to each other."),(0,i.kt)("h3",{id:"expected-during-the-experiment"},"Expected during the experiment:"),(0,i.kt)("p",null,"the topology stays the same, since gateway can ping indirectly (is discussable whether this is ideal or not)\nwhen Broker 0 is leader for a partition then the processing for that partition stops but other partitions should not be affected\nWe can somehow determine in the metrics that they can't connect to each other\nAfter connecting again the affected partition should recover"),(0,i.kt)("h3",{id:"observations"},"Observations:"),(0,i.kt)("p",null,"As expected we see no difference in the Topology. All commands which are send to that partition time out. Other partitions haven't been affected \ud83d\udc4d With the metrics we have we seen that: there is no progress in the partition, the partition is still healthy (which makes sense) and we see a lot of timeouts happening."),(0,i.kt)("p",null,"Unfortunately we need multiple metrics to correlate somehow that it might be due to connectivity issues. I think we can improve here. For example it is not directly visible that one partition stopped processing. For that @Peter Ihme had a good idea and we will add a new panel, which directly shows the current record processing stats. I think this is also useful for exporting to directly see whether we have currently exporting problems. Check the attached image."),(0,i.kt)("p",null,"What else is missing on the metrics side from my point of view:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"a panel which shows me that all requests to a specific partition currently time out."),(0,i.kt)("li",{parentName:"ul"},"metrics for the transport between gateway and broker to better analyze problems like that. Would be nice to have ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/zeebe-io/zeebe/issues/4487"},"#4487")," "),(0,i.kt)("li",{parentName:"ul"},"Liveness and Health stats of the Gateway in the metrics. I think this is currently not supported?")),(0,i.kt)("p",null,"After reconnecting the nodes we saw that the related partition started to process again. Interesting was that it seems that there piled some traffic up and after reconnecting we saw a burst against partition one (partition 2 was disconnected), but this caused no issues."),(0,i.kt)("p",null,"I think was good and interesting experiment again and gave us a bit more insights what else we need."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"feedback",src:n(1877).Z})),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"reduce2",src:n(3568).Z})),(0,i.kt)("h2",{id:"participants"},"Participants"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"@pihme"),(0,i.kt)("li",{parentName:"ul"},"@zelldon")))}d.isMDXComponent=!0},1877:function(e,t,n){t.Z=n.p+"assets/images/feedback-b0105cf21496aacb85b4ded1a34e3b9b.png"},3568:function(e,t,n){t.Z=n.p+"assets/images/reduce2-4f08deb101d3ceaee52e4d64b09e8ddb.png"}}]);